{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "import spacy\n",
    "from torchtext.data.metrics import bleu_score\n",
    "from torchtext.data import Field, BucketIterator,TabularDataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "# CUDA=False\n",
    "CUDA=torch.cuda.is_available()\n",
    "device=torch.device(\"cuda:7\" if CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _transfomer(nn.Module):\n",
    "    def __init__(self,embed_size,src_vocab_size,trg_vocab_size,src_pad_idx,\n",
    "                 num_heads,n_enc_layer,n_dec_layer,forward_expansion,\n",
    "                 dropout,max_len,device):\n",
    "        super(_transfomer, self).__init__()\n",
    "        self.src_embedding=nn.Embedding(src_vocab_size,embed_size) \n",
    "        self.src_postion_embedding=nn.Embedding(max_len,embed_size) \n",
    "        self.trg_embedding=nn.Embedding(trg_vocab_size,embed_size) \n",
    "        self.trg_postion_embedding=nn.Embedding(max_len,embed_size)\n",
    "        self.device=device \n",
    "        self.tranformer=nn.Transformer(embed_size,\n",
    "                                       num_heads,\n",
    "                                       n_enc_layer,\n",
    "                                       n_dec_layer,\n",
    "                                       forward_expansion,\n",
    "                                       dropout,\n",
    "                                       )\n",
    "        self.fc_out=nn.Linear(embed_size,trg_vocab_size)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.src_pad_idx=src_pad_idx\n",
    "        \n",
    "    def _make_src_mask(self,src):\n",
    "        src_mask=src.transpose(0,1)==self.src_pad_idx\n",
    "        return src_mask\n",
    "    \n",
    "    def forward(self,src,trg):\n",
    "        src_seq_len,N=src.shape\n",
    "        trg_seq_len,N=trg.shape\n",
    "        \n",
    "        src_position=(\n",
    "            torch.arange(0,src_seq_len).unsqueeze(1).expand(src_seq_len,N).to(self.device)\n",
    "        )\n",
    "        trg_position=(\n",
    "            torch.arange(0,trg_seq_len).unsqueeze(1).expand(trg_seq_len,N).to(self.device)\n",
    "        )\n",
    "        \n",
    "        embed_src=self.dropout(\n",
    "            (self.src_embedding(src) + self.src_postion_embedding(src_position))\n",
    "            )\n",
    "        \n",
    "        embed_trg=self.dropout(\n",
    "            (self.trg_embedding(trg) + self.trg_postion_embedding(trg_position))\n",
    "        )\n",
    "        _src_mask_pad=self._make_src_mask(src)\n",
    "        trg_mask=self.tranformer.generate_square_subsequent_mask(trg_seq_len).to(self.device)\n",
    "\n",
    "        out=self.tranformer(\n",
    "            embed_src,\n",
    "            embed_trg,\n",
    "            src_key_padding_mask=_src_mask_pad,\n",
    "            tgt_mask=trg_mask\n",
    "        )\n",
    "        out=self.fc_out(out)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=torch.load(\"/mnt/disk1/Gulshan/rnn/translation/vocab_tamil_english.pth.tar\")\n",
    "tamil=vocab['tamil_voc']\n",
    "english=vocab['english_voc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_vocab_size=len(vocab['tamil_voc']['itos'])\n",
    "tam_vocab_size=len(vocab['english_voc']['itos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=50\n",
    "lr=20**-4\n",
    "bs=32\n",
    "src_vocab_size=eng_vocab_size\n",
    "trg_vocab_size=tam_vocab_size\n",
    "# embed_size=512\n",
    "# num_head=8\n",
    "embed_size=768\n",
    "num_head=12\n",
    "n_ecode_layer=8 #6\n",
    "n_decode_layer=8\n",
    "dropout=0.2 #2\n",
    "max_len=100\n",
    "forward_expansion=4\n",
    "src_pad_idx=english['stoi'][\"<pad>\"]\n",
    "writer=SummaryWriter(\"runs/loss_plot\")\n",
    "pad_idx=english['stoi'][\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/conda/envs/gulshan/lib/python3.8/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_model = _transfomer(embed_size,src_vocab_size,trg_vocab_size,\n",
    "                  src_pad_idx,num_head,n_ecode_layer,n_decode_layer,\n",
    "                  forward_expansion,dropout,max_len,device).to(device)\n",
    "_load=torch.load('/mnt/disk1/Gulshan/rnn/translation/checkpoints/6.25e-06_768_12_100.pth.tar')\n",
    "_load=_load['model']\n",
    "_model.load_state_dict(_load)\n",
    "# _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english['stoi'][\"<end>\"],english['stoi'][\"<start>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(model, sentence, tamil, english, device, max_length=50):\n",
    "    model.eval()\n",
    "    # Load german tokenizer\n",
    "    # spacy_ger = spacy.load(\"de\")\n",
    "\n",
    "    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n",
    "    if type(sentence) == str:\n",
    "        tokens = [token.lower() for token in sentence.split(' ')]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    # Add <SOS> and <EOS> in beginning and end respectively\n",
    "    tokens.insert(0, \"<start>\")\n",
    "    tokens.append(\"<end>\")\n",
    "\n",
    "    # Go through each german token and convert to an index\n",
    "    text_to_indices = [english['stoi'][token] for token in tokens]\n",
    "    print(text_to_indices)\n",
    "    # Convert to Tensor\n",
    "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
    "    # print(tamil['stoi'][\"<start>\"])\n",
    "    outputs = [tamil['stoi'][\"<start>\"]]\n",
    "    # print(outputs)\n",
    "    for i in range(max_length):\n",
    "        trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(sentence_tensor, trg_tensor)\n",
    "\n",
    "        best_guess = output.argmax(2)[-1, :].item()\n",
    "        print(best_guess)\n",
    "        # print(output[0])\n",
    "        outputs.append(best_guess)\n",
    "        if best_guess == tamil['stoi'][\"<end>\"]:\n",
    "            break\n",
    "\n",
    "    translated_sentence = [tamil['itos'][idx] for idx in outputs]\n",
    "    # remove start token\n",
    "    print(translated_sentence[1:-1])\n",
    "    output=' '.join([(i) for i in translated_sentence[1:-1]])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 122, 3]\n",
      "0\n",
      "0\n",
      "4\n",
      "3\n",
      "['<unk>', '<unk>', '.']\n"
     ]
    }
   ],
   "source": [
    "sen=\"come\"\n",
    "x=translate_sentence(_model,sen,tamil,english,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape,x[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m2\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(x.shape,x[-1,:].shape)\n",
    "print(x.argmax(2)[-1,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gulshan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
